from sentence_transformers import SentenceTransformer
import os
import sys
import click
import uvicorn
import fastapi
from sqlalchemy import URL
import logging
from tidb_vector.integrations import TiDBVectorClient
from dotenv import load_dotenv
from llama_index.core.llms import ChatMessage
from llama_index.llms.openai import OpenAI
from fastapi.templating import Jinja2Templates
from fastapi.responses import StreamingResponse, HTMLResponse, JSONResponse


# Load the connection string from the .env file
# 设置环境变量
os.environ['OPENAI_API_BASE'] = ''
os.environ['OPENAI_API_KEY'] =''
os.environ['TIDB_HOST'] = ''
os.environ['TIDB_USERNAME'] = ''
os.environ['TIDB_PASSWORD'] = ''



logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger()

print("Downloading and loading the embedding model...")
embed_model = SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5", trust_remote_code=True)
embed_model_dims = embed_model.get_sentence_embedding_dimension()

def text_to_embedding(text):
    """Generates vector embeddings for the given text."""
    embedding = embed_model.encode(text)
    return embedding.tolist()

logger.info("Initializing TiDB Vector Store....")
tidb_connection_url = URL(
    "mysql+pymysql",
    username=os.environ['TIDB_USERNAME'],
    password=os.environ['TIDB_PASSWORD'],
    host=os.environ['TIDB_HOST'],
    port=4000,
    database="test",
    query={"ssl_verify_cert": True, "ssl_verify_identity": True},
)

vector_store = TiDBVectorClient(
   # The table which will store the vector data.
   table_name='embedded_documents',
   # The connection string to the TiDB cluster.
   connection_string=tidb_connection_url,
   # The dimension of the vector generated by the embedding model.
   vector_dimension=embed_model_dims,
   # Determine whether to recreate the table if it already exists.
   drop_existing_table=True,
)
def text_to_insert():
    documents = [
        {
            "id": "f8e7dee2-63b6-42f1-8b60-2d46710c1971",
            "text": "dog",
            "embedding": text_to_embedding("dog"),
            "metadata": {"category": "animal"},
        },
        {
            "id": "8dde1fbc-2522-4ca2-aedf-5dcb2966d1c6",
            "text": "fish",
            "embedding": text_to_embedding("fish"),
            "metadata": {"category": "animal"},
        },
        {
            "id": "e4991349-d00b-485c-a481-f61695f2b5ae",
            "text": "tree",
            "embedding": text_to_embedding("tree"),
            "metadata": {"category": "plant"},
        },
    ]

    vector_store.insert(
        ids=[doc["id"] for doc in documents],
        texts=[doc["text"] for doc in documents],
        embeddings=[doc["embedding"] for doc in documents],
        metadatas=[doc["metadata"] for doc in documents],
    )

def print_result(query, result):
   print(f"Search result (\"{query}\"):")
   result = ""
   for r in result:
      result += f"- text: \"{r.document}\", distance: {r.distance}"

app = fastapi.FastAPI()
templates = Jinja2Templates(directory="templates")


@app.get('/', response_class=HTMLResponse)
def index(request: fastapi.Request):
    return templates.TemplateResponse("index.html", {"request": request})


@app.get('/ask')
async def ask(query: str):
    query_embedding = text_to_embedding(query)
    search_result = vector_store.query(query_embedding, k=3)
    gem_result = print_result(query, search_result)

    llm = OpenAI()
    messages = [
        ChatMessage(
            role="system", content=f"""请你根据提供的知识库参考片段，回答用户的问题。如果提供的参考内容无法回答用户问题，请回复【该内容并未匹配任何内容，请维护相关知识点后再查询】。
            知识库参考片段如下：{search_result}
            """
        ),
        ChatMessage(role="user", content=query),
    ]
    result_content = llm.chat(messages).message.content
    resp = {
        "data": result_content,
        "link": gem_result
    }
    return JSONResponse(content=resp)

@click.group(context_settings={'max_content_width': 150})
def cli():
    pass

@cli.command()
@click.option('--host', default='127.0.0.1', help="Host, default=127.0.0.1")
@click.option('--port', default=3000, help="Port, default=3000")
@click.option('--reload', is_flag=True, help="Enable auto-reload")
def runserver(host, port, reload):
    uvicorn.run(
        "__main__:app", host=host, port=port, reload=reload,
        log_level="debug", workers=1,
    )


@cli.command()
def prepare():
    text_to_insert()


if __name__ == '__main__':
    cli()